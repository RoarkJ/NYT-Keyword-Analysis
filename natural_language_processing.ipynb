{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from config import nyt_key\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_tokenized(year):\n",
    "    months = [\"3\",\"4\",\"5\",\"6\",\"7\"]\n",
    "    \n",
    "    publication_date = []\n",
    "    document_type = []\n",
    "    headline = []\n",
    "    abstract = []\n",
    "    snippet = []\n",
    "    lead_paragraph = []\n",
    "    keyword_1 = []\n",
    "    keyword_2 = []\n",
    "    keyword_3 = []\n",
    "    \n",
    "    for month in months:\n",
    "        base_url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={nyt_key}\"\n",
    "        response = requests.get(base_url).json()[\"response\"][\"docs\"]\n",
    "        for i in response:\n",
    "            publication_date.append(i[\"pub_date\"])\n",
    "            document_type.append(i[\"document_type\"])\n",
    "            headline.append(i[\"headline\"][\"main\"])\n",
    "            abstract.append(i[\"abstract\"])\n",
    "            lead_paragraph.append(i[\"lead_paragraph\"])\n",
    "\n",
    "    data = {\n",
    "        \"publication_date\":publication_date,\n",
    "        \"document_type\":document_type,\n",
    "        \"headline\":headline,\n",
    "        \"abstract\":abstract,\n",
    "        \"lead_paragraph\":lead_paragraph,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # converting data points into one large string value\n",
    "    all_headline = df['headline'].str.lower().str.cat(sep=' ')\n",
    "    all_abstract = df['abstract'].str.lower().str.cat(sep=' ')\n",
    "    all_lead_paragraph = df['lead_paragraph'].str.lower().str.cat(sep=' ')\n",
    "    all_words = all_headline + all_abstract + all_lead_paragraph\n",
    "    words_count = all_words.split(sep=\" \")\n",
    "    print(f\"Total number of raw words: {len(words_count):,}\")\n",
    "\n",
    "    # natural language toolkit\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.util import ngrams\n",
    "\n",
    "    # tokenizing the massive string value and filtering out all stopwords, puncuation and numbers\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    word_tokenize=word_tokenize(all_words)\n",
    "    alpha_word_tokenize=[word for word in word_tokenize if word.isalpha()]\n",
    "    filtered_tokenize=[word for word in alpha_word_tokenize if not word in stop_words]\n",
    "    ngram_two=list(ngrams(filtered_tokenize, 2))\n",
    "    filtered_tokenize=ngram_two+filtered_tokenize\n",
    "    print(f\"Total number of tokenized words after filters applied: {len(filtered_tokenize):,}\")\n",
    "\n",
    "    # creating dictionary with keys=keywords and values=number_of_keyword_mentions \n",
    "    term_freq={}\n",
    "    for token in filtered_tokenize: \n",
    "        if token in term_freq: \n",
    "            term_freq[token]+=1\n",
    "        else: \n",
    "            term_freq[token]=1\n",
    "\n",
    "    # getting the top 100 mentions of all headlines, abstracts, lead_paragraphs and keywords\n",
    "    import math\n",
    "    sort_freq=sorted(term_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_terms_freq=sort_freq[:]\n",
    "    top_terms_dict={}\n",
    "    for each_term_freq in top_terms_freq: \n",
    "        if type(each_term_freq[0])==tuple: \n",
    "            top_terms_dict[' '.join(each_term_freq[0])]=each_term_freq[1]\n",
    "        else: \n",
    "            top_terms_dict[each_term_freq[0]]=each_term_freq[1]\n",
    "            \n",
    "    series = pd.Series(top_terms_dict,index=top_terms_dict.keys())\n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating word mentions with the Natural Language Toolkit\n",
    "* This code tokenizes all headlines, abstracts, lead_paragraphs and keywords into one large string\n",
    "* All stopwords and words containing puncuation or numbers are dropped from the tokenized data\n",
    "* The top 1000 words and their counts are displayed in dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of raw words: 6,126,983\n",
      "Total number of tokenized words after filters applied: 6,839,837\n"
     ]
    }
   ],
   "source": [
    "# series_2020 = pull_tokenized(2020)\n",
    "# series_2016 = pull_tokenized(2016)\n",
    "# series_2012 = pull_tokenized(2012)\n",
    "series_2008 = pull_tokenized(2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new           27846\n",
       "said          16947\n",
       "one           14286\n",
       "york          11345\n",
       "new york      11293\n",
       "              ...  \n",
       "silkier           1\n",
       "chitarra          1\n",
       "gluey             1\n",
       "fleecing          1\n",
       "limoncello        1\n",
       "Length: 1362233, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull data for each year above before running below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2020</th>\n",
       "      <th>2016</th>\n",
       "      <th>2012</th>\n",
       "      <th>2008</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>14456.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>10989.0</td>\n",
       "      <td>17045.0</td>\n",
       "      <td>24894.0</td>\n",
       "      <td>27846.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pandemic</th>\n",
       "      <td>6454.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>6062.0</td>\n",
       "      <td>6727.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>5691.0</td>\n",
       "      <td>6631.0</td>\n",
       "      <td>10153.0</td>\n",
       "      <td>14286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kickier</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rattan</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ketchupy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chitarra</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fleecing</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3435618 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                2020     2016     2012     2008\n",
       "coronavirus  14456.0      NaN      NaN      NaN\n",
       "new          10989.0  17045.0  24894.0  27846.0\n",
       "pandemic      6454.0      7.0     11.0      7.0\n",
       "trump         6062.0   6727.0     92.0    132.0\n",
       "one           5691.0   6631.0  10153.0  14286.0\n",
       "...              ...      ...      ...      ...\n",
       "kickier          NaN      NaN      NaN      1.0\n",
       "rattan           NaN      NaN      NaN      1.0\n",
       "ketchupy         NaN      NaN      NaN      1.0\n",
       "chitarra         NaN      NaN      NaN      1.0\n",
       "fleecing         NaN      NaN      NaN      1.0\n",
       "\n",
       "[3435618 rows x 4 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.concat([series_2020,series_2016,series_2012,series_2008],axis=1)\n",
    "df_combined = df_combined.rename(columns={df_combined.columns[0]:\"2020\",df_combined.columns[1]:\"2016\",df_combined.columns[2]:\"2012\",df_combined.columns[3]:\"2008\"})\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined.to_csv(\"Output/DFs/tokens_raw.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
