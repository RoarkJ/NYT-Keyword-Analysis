{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from config import nyt_key\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_tokenized(year):\n",
    "    months = [\"3\",\"4\",\"5\",\"6\",\"7\"]\n",
    "    \n",
    "    publication_date = []\n",
    "    document_type = []\n",
    "    headline = []\n",
    "    abstract = []\n",
    "    snippet = []\n",
    "    lead_paragraph = []\n",
    "    keyword_1 = []\n",
    "    keyword_2 = []\n",
    "    keyword_3 = []\n",
    "    \n",
    "    for month in months:\n",
    "        base_url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={nyt_key}\"\n",
    "        response = requests.get(base_url).json()[\"response\"][\"docs\"]\n",
    "        for i in response:\n",
    "            if len(i[\"keywords\"]) >= 3:\n",
    "                publication_date.append(i[\"pub_date\"])\n",
    "                document_type.append(i[\"document_type\"])\n",
    "                headline.append(i[\"headline\"][\"main\"])\n",
    "                abstract.append(i[\"abstract\"])\n",
    "                lead_paragraph.append(i[\"lead_paragraph\"])\n",
    "#                 keyword_1.append(i[\"keywords\"][0][\"value\"])\n",
    "#                 keyword_2.append(i[\"keywords\"][1][\"value\"])\n",
    "#                 keyword_3.append(i[\"keywords\"][2][\"value\"])\n",
    "    data = {\n",
    "        \"publication_date\":publication_date,\n",
    "        \"document_type\":document_type,\n",
    "        \"headline\":headline,\n",
    "        \"abstract\":abstract,\n",
    "        \"lead_paragraph\":lead_paragraph,\n",
    "#         \"keyword_1\":keyword_1,\n",
    "#         \"keyword_2\":keyword_2,\n",
    "#         \"keyword_3\":keyword_3,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # converting data points into one large string value\n",
    "    all_headline = df['headline'].str.lower().str.cat(sep=' ')\n",
    "    all_abstract = df['abstract'].str.lower().str.cat(sep=' ')\n",
    "    all_lead_paragraph = df['lead_paragraph'].str.lower().str.cat(sep=' ')\n",
    "#     all_keywords = df['keyword_1'].str.lower().str.cat(sep=' ') + df['keyword_2'].str.lower().str.cat(sep=' ') + df['keyword_2'].str.lower().str.cat(sep=' ')\n",
    "    all_words = all_headline + all_abstract + all_lead_paragraph\n",
    "    words_count = all_words.split(sep=\" \")\n",
    "    print(f\"Total number of raw words: {len(words_count):,}\")\n",
    "\n",
    "    # natural language toolkit\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.util import ngrams\n",
    "\n",
    "    # tokenizing the massive string value and filtering out all stopwords, puncuation and numbers\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    word_tokenize=word_tokenize(all_words)\n",
    "    alpha_word_tokenize=[word for word in word_tokenize if word.isalpha()]\n",
    "    filtered_tokenize=[word for word in alpha_word_tokenize if not word in stop_words]\n",
    "    ngram_two=list(ngrams(filtered_tokenize, 2))\n",
    "    filtered_tokenize=ngram_two+filtered_tokenize\n",
    "    print(f\"Total number of tokenized words after filters applied: {len(filtered_tokenize):,}\")\n",
    "\n",
    "    # creating dictionary with keys=keywords and values=number_of_keyword_mentions \n",
    "    term_freq={}\n",
    "    for token in filtered_tokenize: \n",
    "        if token in term_freq: \n",
    "            term_freq[token]+=1\n",
    "        else: \n",
    "            term_freq[token]=1\n",
    "\n",
    "    # getting the top 100 mentions of all headlines, abstracts, lead_paragraphs and keywords\n",
    "    import math\n",
    "    sort_freq=sorted(term_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_terms_freq=sort_freq[:]\n",
    "    top_terms_dict={}\n",
    "\n",
    "    for each_term_freq in top_terms_freq: \n",
    "        if type(each_term_freq[0])==tuple: \n",
    "            top_terms_dict[' '.join(each_term_freq[0])]=each_term_freq[1]\n",
    "        else: \n",
    "            top_terms_dict[each_term_freq[0]]=each_term_freq[1]\n",
    "            \n",
    "    series = pd.Series(top_terms_dict,index=top_terms_dict.keys())\n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating word mentions with the Natural Language Toolkit\n",
    "* This code tokenizes all headlines, abstracts, lead_paragraphs and keywords into one large string\n",
    "* All stopwords and words containing puncuation or numbers are dropped from the tokenized data\n",
    "* The top 1000 words and their counts are displayed in dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of raw words: 2,237,694\n",
      "Total number of tokenized words after filters applied: 2,548,553\n"
     ]
    }
   ],
   "source": [
    "# series_2020 = pull_tokenized(2020)\n",
    "# series_2016 = pull_tokenized(2016)\n",
    "# series_2012 = pull_tokenized(2012)\n",
    "series_2008 = pull_tokenized(2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new           10736\n",
       "said           5463\n",
       "one            5353\n",
       "two            4023\n",
       "city           3727\n",
       "              ...  \n",
       "abstained         1\n",
       "dishonored        1\n",
       "archery           1\n",
       "blanching         1\n",
       "phoenicia         1\n",
       "Length: 650605, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2020</th>\n",
       "      <th>2016</th>\n",
       "      <th>2012</th>\n",
       "      <th>2008</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>12743.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>9571.0</td>\n",
       "      <td>13508.0</td>\n",
       "      <td>17230.0</td>\n",
       "      <td>10736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pandemic</th>\n",
       "      <td>5874.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>5775.0</td>\n",
       "      <td>6016.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>5075.0</td>\n",
       "      <td>5897.0</td>\n",
       "      <td>7581.0</td>\n",
       "      <td>5353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>throb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gruffly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capraesque</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hooverville</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abstained</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2496524 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                2020     2016     2012     2008\n",
       "coronavirus  12743.0      NaN      NaN      NaN\n",
       "new           9571.0  13508.0  17230.0  10736.0\n",
       "pandemic      5874.0      6.0      9.0      5.0\n",
       "trump         5775.0   6016.0     58.0     30.0\n",
       "one           5075.0   5897.0   7581.0   5353.0\n",
       "...              ...      ...      ...      ...\n",
       "throb            NaN      NaN      NaN      1.0\n",
       "gruffly          NaN      NaN      NaN      1.0\n",
       "capraesque       NaN      NaN      NaN      1.0\n",
       "hooverville      NaN      NaN      NaN      1.0\n",
       "abstained        NaN      NaN      NaN      1.0\n",
       "\n",
       "[2496524 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.concat([series_2020,series_2016,series_2012,series_2008],axis=1)\n",
    "df_combined = df_combined.rename(columns={df_combined.columns[0]:\"2020\",df_combined.columns[1]:\"2016\",df_combined.columns[2]:\"2012\",df_combined.columns[3]:\"2008\"})\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined.to_csv(\"Output/DFs/tokens_raw.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
